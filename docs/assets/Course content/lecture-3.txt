See the...

Today's lecture topic will be about regularization and optimization.

Which are two very important concepts more broadly in deep learning and machine learning.

But especially important for computer vision. And we're going to start with a recap from last week and discuss some of the topics.

Topics that we discussed last time.

We really honed in on this idea of image classification as a core task in computer vision.

What this task is, is given an image as input, you try to map this image to

A label inside of a set of labels. So here we have five different labels.

Cat, Dog, Bird, Deer, and Truck. And the goal is to assign the correct label to the input image.

They're creating some model or some function that takes an image as input and outputs the specific label here.

And we also talked about a lot of the challenges for classification.

One of the main challenges is shown in the top left here, but it's this idea of the semantics.

There's a semantic gap between what we as humans perceive in the image, which is the cat, and what it's actually represented.

As in the computer, which is this grid of pixel values where you have this multi-

You have a multidimensional array or tensor and you have discrete values for each of the pixels.

This is very different from how we're perceiving the image and deciding that this is a cat.

So to map from this complex numeric representation into one that we humans understand is

That is the core challenge here. But also there's challenges surrounding the images themselves. So if you look at something

Something like the illumination of the scene. So here you'll have different pixel intensities based on where the

Where the lighting is in the scene. Also, you could have certain parts of your object are in the shade.

And harder to see. Cats, by nature, are very deformable. So you talk about deformable objects. They can move around and .

They can go down and twist and bend in different ways, so they won't always have the same shape. And this can prove challenging if you're trying to design an algorithm.

There's also the challenge of occlusion. So you could have a cat that's like hiding underneath the-

The couch cushions here. But we as a human can clearly tell this is a cat because of the tail, how it's sort of .

We're sticking out at the end here, and then the way that cats behave, we can infer that this is a cat.

You'll also have things like background clutter where the object could blend into the background. So we need to account for this.

For this somehow as well. And finally, there's this idea of intraclass variation where

Different objects in the same category can look very different from each other, but we still need to group them all into this.

So here are a lot of just the challenges of recognition and why it isn't such a

It's a simple problem where you can just sort of write if-else rules to account for everything.

And just simple logic to classify. So if logic's sort of thrown out the window, you can't just create these

These logic rules. How do you actually create a classifier? Here's where we talked about data-driven.

And we talked about basically the simplest machine learning model, which is this case.

This is the Pay Nearest Neighbors model. And the idea is that you look at, for a given data,

What are the existing data points in your training set that are

are very close in distance to your new data point coming in.

The one nearest neighbor case, this just results in, you find the closest data point.

You assign it that class label. And you can also look at multiple nearest neighbors where you're

Assigning the most common class label among those nearest neighbors. So we talked about these two different approaches.

We talked about how you ideally don't want to split your dataset into train and test.

But you can do train validation and test so that you can use this validation set here.

to actually help you choose your hyperparameters. So the main hyperparameter...

The first parameter for k nearest neighbors is this k, one or five in these examples. And what we showed is an example.

So this is an example where you're plotting what is your accuracy on this validation set here.

Consider the different K values here, and you would choose the one that has the highest accuracy.

So this is how you'd use the validation set, and then you would reserve this test set for, OK, how does your model do on completely new data?

It's never seen before. That would be the purpose of the test set. This is all just recap.

There was a bit of confusion about distance metrics. We put a post on Ed that explains this in more detail.

But we talked about two different distance metrics, the most two commonly used ones.

In machine learning, which are the sort of Manhattan distance and or L1 distance and L2 distance.

L2 distance or Euclidean distance. L2 distance is like, if you imagine just this

The straight line distance, sort of how we think of distance in everyday usage of the word.

And then Manhattan Distance is this idea where you can only sort of traverse

It's left and right and up and down in this diagram. You can't move diagonally. So specifically looking at, just one quick example.

Quick example here, the reason why all these points on the line are the same distance from the origin

ReLU is because you can't move diagonally, so you have to move in this case up 0.5.

And to the right, 0.5, so the total distance is 1, whereas, you know, here you're just going a straight line, but it's...

One also is the same distance here. Whereas in the L2 distance,

All the points equidistant from the origin here form a circle because you can just go in the direction.

So this is maybe a brief explanation. The final thing we...

What we sort of honed in on last time was this idea of a linear classifier.

So the basic idea in the basic setting that we did is we have an image which is

Which is, say, width 32 and height 32. And there are three pixels.

We have the spatial values for each of the spatial locations in our image representing the red, green, and blue.

Intensities forming the color and the idea is we take this

Array of numbers for our image and we flatten it out into an array of just 3,000 different numbers.

Here's 3072. And then we're multiplying this vector by our weight matrix.

And the basic idea is if we have a weight matrix W,

That has, you know, a height here of 10 and then...

And then the width is 3,072. We're multiplying each of these rows by our .

Our input sample x, and this will give us 10 resulting class scores.

So oftentimes we'll add a bias term as well, which would just be one bias term for each.

So this would be a size 10 vector here. And we also talked about

I'm going to talk about three different ways you can view or think about these linear models. One is the algebraic viewpoint.

Which I described here where each row is represented sort of independently representing.

So if you're presenting the class and you multiply it by the input vector x, you get your score.

And you add the bias to get your final score. And you do each row sort of independently in this sense.

You can also view these learned class

Weights here as templates where if we then sort of re-ravel the

So if we could get the vector into the original shape of the image, we could plot the intensity.

So we have 10 cities here and understand what is sort of the...

Template per class, which is what this visualization represents. And then the final way you

The way you can think about it is in a sort of geometric viewpoint where each of these

Rows in our weight matrix are represented by these lines here in...

In our input space, and specifically the line is where we set

We get this equation to zero, which is the decision boundary, so this forms the point

To the point where above the line you can have a positive score and below the line you would have a negative.

That's a negative score for the class. So these are sort of the different viewpoints.

So for how you can view these linear models, they're all doing the same thing. And one nice thing about the geometric

And the second metric viewpoint is that if you visualize your data, like say you want to classify

Blue versus red here. It's very easy to tell that you can't draw a line that perfectly separates.

So it's kind of a nice way you can gain intuition about what is possible.

It's impossible for a linear model to do. Okay. I think that's sort of the high-level...

I'll actually be going into a little bit more.

More detail on sort of the new content for this lecture now.

I just wanted to pause briefly. If anyone had any questions about what we discussed last time or at the beginning of this lecture, feel free.

Feel free to ask.

Technical viewpoint, is this the same as sort of running k-nearest neighbors, and this would be...

Maybe one of the neighbors that you're comparing against. Are they mathematically equivalent? No they're not.

They're not the same because these templates are formed from this line.

But you can still calculate the templates based on.

Based on this sort of, they would represent more like, if we see in this diagram.

There's the line pointing in the direction of the class, so it would be sort of representing this point.

Yeah, so the question is, how did we get this 3,072 number?

So the idea here is that if the height of our image is 32 pixels

And the width is 32 pixels. And then each location in the image is represented by three values.

Red, green, and blue pixel intensities. We would then get 32 times 32 times three total values.

We use to represent the entire image, and that's how we get this 3072 number.

So, here's a very specific example of a .

The linear model here. And when we multiply our input x by

By our weight matrix W, we get the resulting scores for these different classes.

And you can see that for CAD it's not doing so well because CAR has a higher score and we want the highest score.

for the correct class. Also, here on the second example does pretty well because it's

But then in the frog example, it sort of gets it completely wrong, where it's by far the lowest score of the .

So intuitively we can tell that these scores are not very good, but how do we sort of

How do we mathematically formalize this intuition? And how do we determine how good a given model is?

This is the idea of a loss function, which specifically tells you how

How bad a classifier is. So given a dataset of examples where

We're indexing with this letter i. We have x, i is each.

Each of the training examples, Yi is each of the training labels. We can compute the loss over our-

Our entire data set where we calculate this loss for each training example.

For example, by sending it through our model here, which is this f of x i w, we get our label.

And then we compute it compared to the ground truth label, YI. And we just take the average.

So this is how we do this. We talked about in last lecture.

In the last lecture, the Softmax loss or the Cross-Entropy loss, which is the most commonly used loss for class

And so I won't discuss that again in so much detail here, but basically...

Basically, it's a very high loss when you predict low probability of the

The correct class is a very low loss when you're predicting the correct class at very high probability.

So this is what I just explained.

This is all contained within what we call the data loss. So this is...

A loss that tells you how well do the model predictions match our training data.

We want this to be very low. And if it's very low, it means our model's fitting our training data well.

The second component, which I'll discuss today, which is this regularization term of the law.

So what this does is it's intended to

So it actually does worse on the training data.

But the goal is to make it do better on new test data or unseen data. So worse on training, but better on

That's the point of regularization. And we'll go over a lot of the intuition for how to think

Think about it in the next slides here. But the high-level goal is to do worse on the training data, but then

But then better on test data or just unseen data. That's the point of regularization.

Yeah, so we're computing the loss on each of the eye training examples.

The loss of the ith example uses the x, i, and the y, i.

Does that make sense? I mean, you could not have an idea, but...

Yeah, you normally don't have a different loss for each eye.

That's what you're asking. Yeah, yeah, yeah. So it's just, you could just, we described a lie.

It's the loss for the iTraining example, so we're just using it here, but yeah, it could be out.

So for regularization, people usually have this intuition when thinking about

This is sort of like a toy example and the idea is we want to fit some function.

So we have a function to these points, where our input is x and our output is y.

You have two different types of models, F1 and F2. And you're trying to decide which of these is better.

So F1 goes through all of our data points. So the training or the data loss

Loss will be very low, because it's basically doing it perfectly.

It doesn't go through every point perfectly, but intuitively it feels like probably F2

That too is a better model when we're now testing on new data we've never seen before.

So regularization sort of captures this intuition of you don't want to overfit your data so hard.

And you might actually be better off with a model that fits the data less, but is either simpler.

So it's simpler or has some other properties that make it a better choice.

You know, ask, OK, how are these models going to do on new data that's within our same distribution?

You can find that F2 does a much better job at modeling. So here's what's doing better on the unseen data.

So I think there's also an intuition in this previous example.

This is an example that's demonstrated very well where we're preferring simpler models, where it's sort of like Occam's Razor, which is this.

There's this idea in philosophy and also scientific discovery where if you have multiple competing...

You should go with the simplest one first, and then if you know for sure that's wrong, then you can start trying out.

So you're trying out more complicated ones as you go. But this is maybe also some intuition you can have for why regularization.

It can be useful. Okay, and then one final thing about this equation that I didn't

I didn't touch on yet is this lambda parameter here. So this is the regularization strength, which is enough.

This is another hyperparameter. So we might use training validation sets to set what is the

The optimal lambda here as well. But the basic idea is we can set this to a floating point between

I guess zero and infinity, where zero would be basically, there is no regularization.

And up to infinity, you have a really strong, progressively stronger regularization.

So it's very much a tunable knob you have for determining how much

Much you want to prevent the model from fitting to your training data.

And I'll go through some simple examples now of regularization. So here we have L2 regularization.

So you have two regularization, which basically what you do is you have your weight matrix

You square each of the terms in your weight matrix and then you sum them all together. That gives you your

To your score here that you then multiply by lambda and you add to your total loss, that's L2.

L2 regularization. L1 regularization is very similar, but instead of squaring, you're taking the absolute value.

So in practice, there are some differences between how these two

L2 regularizations perform when you're training models. So one of the things that happens with L2 regularization

Because you're squaring each of the values, when you have a really small value, it gets squared.

You know, 0.001, you square it, it becomes even smaller. So L2 regularization allows

allows for sort of these really small values close to zero because you then square them so they become even smaller.

So your penalty here is very low if you have these very small values with L2.

L2, whereas L1, you're not squaring it. So it's sort of just whatever the baseline value is.

It's not like it's getting smaller before you're computing this regularization term.

In practice, what this leads to is L1 regularization, you get a lot more values that are zero actually in your weight matrix.

L2, you can have, generally it's more spread.

We're spread out where you have values that are small but non-zero because the penalty becomes so small.

It seems pretty clear to you why L2 prefers sort of spread out weights that are

But why does L1 prefer sparse vectors? So I think the way to think

The way I think of it is that if a value can be zero and your performance is roughly the same,

Then this would push you towards zeroing that value. Whereas for L2, what you might-

What you might have is the value just becomes very small but non-zero because of the squaring.

So the question is can you talk about what is pushing towards a zero value mean?

We're going to talk about more how we use this lost term, but the basic ideas were

It means we're trying to minimize it. So we're trying to minimize the loss or minimize the error of our model.

And if we have a term here which is giving us positive value...

So if we have the values that it's sort of not affecting the model performance and the data loss, we will...

We'll sort of remove those through the optimization procedure. It's sort of a trade-off. You're trying to optimize the joint.

Sum of the regularization term and the data loss term. So if your data loss isn't changing much,

But you're able to go lower on the regularization term, you'll get a more optimized model, so it will.

It will be preferred based on trying to minimize the overall term.

So I think we'll also touch later in the course about much more complex forms.

There are various forms of regularization where they're all doing this basic idea of worse on the training data to do better on the testing.

Test data. But some of them you will even change the layers of your model.

They actually get pretty complicated. This is like an ongoing research area of how to regularize models.

There's new papers each year, so lots of stuff here. We'll only cover a small subset.

So to summarize, why do we regularize models?

The first is, you know, it allows us to express some sort of preference over weight.

For some reason in our problem, we think the solution should be spread out or should contain a lot of sparsity where a lot of

If a lot of the values in the weight matrix are zero, we might prefer one set of regularization, L2 versus L1.

One over another. It also can, depending on how we're regularizing, make the model similar.

So that it works better on test data. So it could simplify the model if we're, say,

Heavily regularizing really high polynomial terms in our model, for example,

What I showed earlier. And something we won't touch on in too much detail is especially L2.

L2 regularization can actually improve the optimization process because if you imagine like

Like the squared is like a parabola. So if you're plotting y equals x squared, it's a parabola.

And these are convex, so you get a lot of nice optimization properties where there's a global minimum.

We won't touch on that in this course that's beyond the scope, but know that for certain types of optimization,

And the regularization actually helps train the model faster, too.

I guess I have a question for you all, and what we'll do is you'll do...

One if it's W1 and two with your hand if it's W2.

Which of these two weights, W1 and W2, would

The L2 regularizer prefer. So we have our input x.

When you multiply it, you do the dot product with the weights, you get the same score. So you get a score of one either way.

And here's where the data loss would be the same. And we're trying to determine which of the

Weights would our regularizer prefer. So go one if you think it's W1 and go two if you think

Do you think it's W2? All right, lots of twos. Yeah, it's W2 because, as you said, it's more spread out.

You're going to be squaring each of these turns, so it's one-fourth, you square it, it becomes one-sixteenth, you sum it all together.

One-fourth is the total regularization term here, and then here you square it.

So it's one. So it's four times lower in terms of the regularization loss.

And as you said, the intuition is you like more spread out weights. And then here's another question.

Which one would L1 prefer now? So you can do 1 if it's weight 1 and 2 if it's weight 2.

Okay, we got a lot of ones.

So this one's actually a bit of a trick question. So what L1 regularization is, you sum each of the terms.

So they'll both be summed to 1. In practice, you probably would see this one because as we said, the sparsity.

But in terms of a loss standpoint, these two weights would actually be equivalent.

L1, because one is just the sum of 0.25, four times.

And then the other one is just one. So they're both sum to one. And so the actual

Regularization term is the same for these. Yeah, okay, so what's an example where L1 would be?

One would be preferred if this is like 0.9, for example.

So just to recap, we have a data set of XY pairs, and we have

We have some way to calculate scores for each of the classes.

In our case, it's just a linear model. You're doing a matrix multiply.

The loss for each of the eye training examples and the Softmax loss.

The last loss, which we discussed last time, is you exponentiate each of your scores.

And then you divide by the total sum of the scores. So you exponentiate to make them all

Make them all positive. And then you sum to get a probability distribution.

The final values in this all sum to one, and you have a score for each class, and you take the minus log of your

So this is the probability of the correct label which is given here.

And the full loss is you just run this over each of your training examples.

Calculate Li for each of those, and then you add your regularization term here.

Depending on what is the weights of your model. Why do we use Softmax in general?

So Softmax is great because what it does as a function is it converts any set of

Set of floating point numbers into a probability distribution where they will sum to one.

And depending on the value of the score, that will translate to the relative probability.

So if you have a really high positive number and everything else is very low negative, you'll have

We have nearly one for Softmax and zeros almost for the other values. So it's nice because it...

Because it converts any list of floating point numbers into a list of probabilities.

Socknext is based on the values of the list.

So the question is that you can view the regularization we talked about, which is L1, L2.

That's a way of regularizing based on the magnitude of the weights, which is true.

And how does that translate to simpler models? So I think in L1's explanation is actually pretty simple because

Because if we prefer, say, terms that have a lot of zeros in it,

It's basically a linear model with fewer coefficients. So that one is actually, I think, relatively straightforward.

But I think in general, regularization is not always going to give you a simpler model. It depends on how it's used.

So, for example, in the diagram we showed at the very beginning here.

Here you could imagine that you have L2 regularization or L1 regularization.

Where you're penalizing more the higher degree polynomial terms of your function. So in that sense, it's pretty

It's pretty clear how you could design regularization to prefer a simpler model. But it doesn't always need to be that way.

Really what it is is this idea of doing worse on the training data to do better on the test data.

That's not always going to give you a simpler model. And in fact, there are many types of...

regularization like Dropout that actually make your model more complex, but give you better performance.

on the test data.

Cool. So now that we've talked about how we can calculate how

How good a given W is based on the training data and this

This regularization term. The question now is, how do we actually find the best W?

And this is what optimization is, which is the second half of today's lecture.

So I think when people describe optimization, they will usually use this idea of a lost landscape.

What you can think of as like a normal landscape like on planet Earth where the up and down.

The up and down vertical or z-axis direction is the loss. So this is the value we're trying to minimize.

And then say in this example you have two parameters in your model, which is sort of the x and y direction of where you

Where you are in this landscape. The idea is you're just like basically a person. You're walking around this landscape.

And you're trying to find what is the smallest or lowest point in the entire landscape. I think one of the reasons...

This is a very commonly used analogy. A little bit falls apart is because, as humans, we can just

We can just see visually. We can look into the distance and see what is the lowest point of the valley.

This analogy is actually pretty accurate if you think of the person as being blindfolded. They don't have access to any visual information.

They can only feel sort of the earth where they are right now and understand what is the slope of the ground.

I think if you view it in that lens, this analogy actually becomes extremely accurate.

for how we're trying to find the best model and we have this complex landscape of

Different loss values depending on the parameters of our model, which translate to the location of the person.

So how can you find the best point?

We could go with like a really simple idea, which is maybe a really bad...

You know, bad idea, but it could work.

So here we go.

So here it's just basically a for loop where we're trying 1,000 different values of w randomly.

And we're just choosing the best one. So obviously not very mathematically rigorous, but you know you

You will do better than Random Baseline. And if you had nothing else to go for, maybe

This isn't so bad. You would get like 15.5% accuracy on the

The CIFAR 10 Dataset, which is the one I showed earlier with the frog and the car and things like that.

like that.

with the ten different categories, but it doesn't perform very good.

The state of the art on this dataset is basically solved through modern deep learning. You get 99.7% accuracy.

So clearly it's not bad, but it's also, I wouldn't say, particularly good.

Strategy number two, which is sort of what I maybe...

Explained a bit earlier is this idea of following the slope. So for this,

You can imagine you're like blindfolded on the landscape and you're feeling the ground underneath you.

And you're thinking, OK, which way is the slope of the Earth pointing me?

And I should walk in that direction at all times. This basic idea is the fundamental way

This is a way in which we train all the models in this course and in which basically all deep learning models are trained, where you're

You're feeling the location of the current place in the lost landscape and you're going down the hill.

This is a very intuitive way to explain it. We'll now go over more of the math behind it, but this is what you should be maybe

Maybe visualizing in your head. So how do you actually follow the slope?

So in one dimension, I'm sure you all are familiar with the idea of a derivative, which

Which in calculus we can think of as this limit h definition where we add a very small number.

So we add a number to our current location. We calculate the value of the function at that new location.

We subtract the current location, and then we divide by the step size.

For h to approaching zero, this gives us the derivative of the

of the function at that point. Now, this is for 1D, but in multiple dimensions you

You use the gradient, which is where you're calculating essentially this limit definition for each.

So you'll have a different derivative for each of your

And you get a vector instead.

This gives you the direction along each dimension. So you can actually calculate the slope in the dimension.

You can change the dimension by taking the dot product of the gradient with the direction and

Specifically, the direction of the steepest descent or down the hill is the negative gradient.

So the gradient points up the hill, the negative gradient points down the hill.

That's the direction we should be traveling if we're trying to get to the bottom of this lost landscape.

Maybe what are some ways you can calculate the derivative? A really simple one is you could just actually try to use the limit

So you limit each definition with a very small h. So you add 0.0001.

You actually can compute, you know, the last two digits of loss changed slightly so you can compute the

The difference divided by the step size, and you can get an approximation of your derivative here.

You could actually do this for each of your values in your w. You just do this procedure.

But it has a few problems. It's very slow because you just need to loop through each of the values.

It's also approximate, so you're not even calculating the actual derivative, and especially with floating

Loading Point. Arithmetic, you can get pretty significant errors here.

Not really preferred, but this basic idea or intuition of what we could be doing is to calculate the

The derivative this way. But really we have the loss as a function of double.

So we know how to calculate the scores to calculate, to get our.

We can compute our loss, which is given by our function for our model.

Compute the total loss with the regularization terms as well. And this entire loss...

Loss is a function of basically Ws, the Ws, the Xs, and the Ys.

So you have your w-matrix, you have your xi's and yi's, and then you have this formula with maybe some logs and exponents.

But fundamentally, this is a function of .

W, X, and Y, and we specifically want to

We want to calculate the gradient, which is given by this Greek letter nabla, of our loss with respect

So we can imagine our X and Y.

i's are held constant, and we're trying to calculate the derivative just with respect to the weights.

So to do this, we can just use calculus, use the chain rule, use the different...

There are different methods we've learned for calculating derivatives based on sort of complex equations.

Exponents are not so complex, but you need to have some logs and exponents and chain rules here to solve it.

So this will be an exercise in the homework, so I won't go through step-by-step how to do it now, but it is relevant.

It's relatively straightforward. I think conceptually it should make sense to you all how you do this. You assume the x and the y's are constant and you solve.

What is the derivative as you change w? So now we actually have a wave.

This is a way where we can calculate w based on our, sorry, dw.

That's the gradient of W with respect to our data and the current W and whatever.

Whatever our loss function is, which is how to compute the error.

Summary, so you could do a numerical gradient, but it's approximate slow and

The nice thing is that it's very easy to write. You just add a really small h, take the difference, divide by h.

The analytic gradient is nice because it's exact, it's fast, but you could potentially

Potentially, if you're creating a new gradient from scratch, like new code to calculate from scratch, you could have an error in it.

If you are doing this, people normally will have a gradient check, which is where they try the H.

version where they have a really small h-value, and then they make sure that it's around the same neighborhood, and that's a good way to make it.

Make sure you don't have any bugs in your code. So you'll be doing gradient checks in your homework assignment.

Assignments to make sure your implementations are correct also.

We often say we want a loss function that's differentiable because then we can calculate the gradients.

But if we have a better loss function somehow, and we can't analytically

We can analytically calculate the gradient, but we could use this h kind of numerical method.

Could we do that? I think, in general, it's hard to construct a better loss function

A function that would be non...

Non-differentiable. You could possibly, though, and if there is just a true loss function that is

The best for your case, but it is not differentiable. You could go with this approach and it may work.

I think it would struggle if, for example, your loss is just truly

So if it's basically non-differentiable across all points, and it's basically like a cluster of non-connected points, then

Moving in the step of steepest descent wouldn't really get you necessarily

It's really at your best solution if they're not well connected and forming sort of this geography.

It could work, but I would think that if your loss is non-differentiable across most of the domain, then...

Probably you wouldn't be able to use these approaches to find.

Find the, uh, the bottom point.

I guess TLDR of the explanation is that if your function's convex, then it

Works very well with this sort of gradient descent or steepest descent type of approach.

If you have a non-differentiable, non-convex function, probably this approach won't work as well because you're not going to be

It's not necessarily error-prone if your code is perfectly good.

But maybe you have a mistake in your code and it's hard to tell right away.

The limit h definition is very easy to code up, right? You just set h to be a very small value. You run it through your function.

And then you add a very small amount. So that's less error prone for implementation.

For implementation. Not more error-prone if it's working, correct.

Okay, so now I'll talk about this fundamental algorithm for optimization called gradient

And the basic intuition is what we already explained before. We calculate the slope at each point.

When we're on our lost landscape and we take a step in the direction downwards towards the bottom of the lost landscape.

So what we do is we calculate the gradients of our weights.

Given the loss function, the data, and our current weight values, this tells us

So that tells us how much we should change each of the weights to go down the slope. And then we have to have a step size, so how far down.

So if you go down the hill, I'll be taking a step in the direction.

The minus sign here and the step size times the gradient. So this is basically...

This is basically what gradient descent is. You're calculating the gradient at each step, and you're moving a fixed direction.

In the direction of the negative gradient down the hill.

So, given a concrete example here, so instead of this being like a 3D loss landscape, often people

Some people will visualize it like this where we're sort of looking down at the landscape and purple would represent that.

And the highest points in red would represent the bottom or the valley here. And we can imagine we have our original

Our original W, we can calculate the loss. We know the direction of the slope, the negative gradient direction.

And this arrow might represent the fixed step size that we talked about before we were taking.

Taking a fixed step size in that direction.

Yes, you can see it's fixed step size, but as the-

Gradient becomes smaller. We're still multiplying it by this fixed step size, so the effective

ReLU, Softmax.

It actually does become smaller because the gradient is smaller near the end where it's flat.

Or into the n-word.

Where it's more flat. So this is what it looks like.

So the question is, when we step down, how do we...

How do we know when we're going to stop? Well, I guess in this formula, you just keep looping forever.

So this was probably not the best. Normally you have a predetermined number of iterations.

That you run it for. Or you can look at if the loss is not significantly

It's definitely changing by a fixed amount also. You can have like a tolerance for how much you're expecting the loss to keep.

And if it's no longer decreasing, you know, it's only decreasing by 1e-5.

5, or 1e-9, maybe you stop there because it's good enough.

So those are the two ways you can determine when to stop, is the fixed number of iterations or stopping

Topping criteria of how much we're not really improving that much anymore.

Okay, so now I'll talk about the sort of most popular

There's a popular variant of gradient descent, which is called Stochastic Gradient Descent.

And when we talked about gradient descent before, we talked about calculating the loss of our weights by

By summing over our entire training set, the loss of Li for each i in our entire n.

But this is potentially a lot of computation if we have a very large

So what Stochastic Gradient Descent is, is it basically

Basically, now, instead of looking at the entire dataset, we're looking at a subset each time, which we call a mini-batch or a batch.

They're a batch of data. And so here, if we look at the code, it's like we're sampling 200

We have 256 data points from our dataset, so the batch size is 256.

We evaluate the gradients of this 256 subset of our data set.

And then we do the same thing as before. So the reason why it's called Stochastic Gradient Descent is because we're sampling a

We're doing a random subset of our data set each time we're running the algorithm, each step of the algorithm.

So this is Stochastic Gradient Descent. You're basically running it on a random subset each time.

And in practice, people won't just sample it completely random.

I'll make sure to get through all the examples in their dataset and then sort of loop around again.

And that's called one epoch of training where you do all your data samples once in a random order.

OK. There are some problems with gradient descent.

Or Stochastic Gradient Descent. So this visualization is sort of the same type as

The colored one I showed before, we were looking down at the Lost Landscape, but these curves are called the Levels.

It's a level set where it's a set of points where the loss is the same on all of them. So this is another way of visualizing sort of...

Very popular way to visualize top-down looking at the loss, but it's without the colors.

And so you could imagine that you have this phenomenon where it's like a

It's like a really narrow valley where it's really steep along the sides and you're trying to traverse the center of the valley.

Grading Descent actually does run into issues here.

Does anyone have any ideas for what could go wrong? Yeah, so one of the things you could do is

You just overshoot where you're sort of moving up and down along this direction.

And if it's steep enough and your step size is large enough, you might actually oscillate out of the valley.

So you can imagine if your step size is very large and this is really steep, you're actually going to be gaining

Like you're moving out and out each time because you always have this fixed step size.

If it's steep enough, you could just bounce out of the valley. That actually does happen if you're learning rate.

It's too large. That's one thing that can happen. And then also, even if your learning rate's not too...

If your step size is not too large, you can have this phenomenon where

Where you're sort of jittering because the gradient is much larger in the steep direction.

So you're sort of jittering, but you're not making very much meaningful progress towards the actual center because you're spending all this

Most time oscillating back and forth, up and down. So this is a pretty big issue with just .

Default, SGD. And then mathematically, just an aside,

The loss function we consider here to have a high condition number, which is the ratio of the largest

It's the smallest singular value of the Hessian matrix, which is the second derivative. So you can imagine like...

The second derivative along this up and down direction is very high, but then side to side it's very low.

Because it's very flat. So that's what causes this phenomenon.

All right. So one of the things we also might have an issue with

What happens if the loss function has a local minima?

Or a local minimum or a saddle point. So, for example, here.

Here, for just the very end of this curve, it's completely flat.

Imagine we're moving down the hill here. We would just get stuck.

We're stuck here because it's flat, and we wouldn't be able to progress any further because when we take the gradient here, it's zero.

So this is actually a pretty big issue where it'll get stuck either in a local minimum

The whole minimum, because once we reach here, we don't really have any direction to go. The gradients is zero.

We'll just sort of oscillate back and forth here. And then here, it could actually get stuck on this bottom.

It's a bottom example because the gradient zero here, even though if it went a little bit further, it could go down significantly more.

Yeah, so the question is, maybe we can change the way we're doing this.

Steps. Maybe we could use the Hessian to determine the direction we go. We actually do have a brief slide.

Talking about the sort of Hessian style approach at the very end. That's not very commonly used in deep learning.

But the short answer is yes, there are going to be actually several ways in which you can account for this that we're going to go into.

So it's a good question. We'll get to that.

OK. So I think one of the other things

One of the other things that you might not know is that empirically saddle points are actually much more common as you move to higher dimensions.

So as your weight matrix gets larger and larger, you're more likely to find

Find these saddle points, and there's this paper describing the frequency of them. If you don't know a saddle point, it's called

It's called a saddle point because it's shaped like a saddle, like on a horse. And at the center of the saddle

The gradient is actually zero in all directions, so it's like the bottom of this.

And at the top of this sort of curvature. And so in both the x and the y direction.

So you could get stuck here despite being very close to going significantly down the...

The Lost Landscape on either side. So this is also a pretty common issue with SGD.

We're at these saddle points, and as we move to higher dimensional spaces, or there's equivalent to models with more

There's more parameters. This is more and more common. This is a big issue.

And then a final issue with SGD is that we are

Sampling a subset of our data each time, right? So we're not looking at the whole, this represents the .

The entire loss across all the data, but we're looking at just a subset each time. So we'll actually have somewhat .

But noisy update steps, because we're not looking at the entire data set. So we'll sort of be stepping.

Stepping towards the center, towards this sort of local...

Minimum that we're trying to reach here, but each step doesn't go directly in that direction. So there's some noise in how...

And how we're progressing because we're subsampling the dataset.

Okay, cool. I think to summarize, these are the main issues.

And there's a pretty neat trick you can do where you just basically add momentum.

And you can really think of this as the same way as like if you have a ball that's rolling down the hill where it gains momentum, it's actually very.

Very similar to how it's modeled in terms of the physical properties. So it's a good way to gain intuition about it.

At the very least. So you can imagine that it could help with the you have these local minimum.

Because if you're rolling down with enough velocity, you'll be able to come out of it. If you have the saddle points or the .

For the flat point here, the model has been rolling down the entire hill so it won't get stuck.

Stuck here anymore. We'll continue. Also, if you have this poor condition,

You will still have maybe some oscillation.

The nice thing is that it will accumulate speed in this direction to the right because it will have

We'll have multiple steps that keep going that way, so it'll gain faster and faster towards the center here.

So this problem, finally, it can also help sort of average out some of the noise with the gradients.

Because they all sort of have a direction in common, which is towards this.

That's a minimum here. And so as you're computing the momentum, it's-

It sort of builds on itself and it will converge faster because it sort of the noise.

This is accounted for by looking at the direction they all share in common.

Which is included in the momentum. So let me show you how to actually do it, but this is sort of the .

The general intuition for how momentum works.

GD here, we have our mini-batch x. We're computing the .

And the gradient, which is DX, we have the learning rate or the step size. If you multiply and then we do the negative because we need to go

We're going to go down the hill. This gives us our new x. So this is SGD for with momentum.

So in momentum, we're now updating by this velocity term. So instead of updating by the gradient,

Gradient at the specific point, we're updating by the velocity, and the velocity at a given time step is given

It's given by the previous velocity plus the current slope. So this is sort of how you

And you have this row value, which is the momentum, the actual like how much momentum you want to have.

And if you have it very high, then your new velocity is more dependent on the previous time step.

And this sort of is a running average, therefore, of the last

Past gradients depend, and the momentum term here gives you how much to weight the past versus the present.

So now we're updating by this, and we still have this alpha, which is the step size. So it's actually a very simple change.

You just are now computing the velocity, which is a function of the current velocity.

So I think I'll pause for questions here.

This is the explanation of momentum, and maybe I could also recap briefly.

And that's basically how it resolves all these issues we saw. So now that you're adding momentum in the past,

Over the past gradient steps, you could see how it would keep continuing along this direction.

Depending on your row, if your momentum is very high, it would keep going and be able to account for a very

Large sort of hump here with the local minimum. Also, it's very good at sort of these saddle points because it will just continue.

So, we're going to continue along the direction in which it was going previously for a significant amount of time and poor condition.

If we're having cumulatively going to the right upon each step, the momentum will

We'll also be consistent there and build up. And then if we're oscillating significantly here,

Here, it will move less in the direction because the values will .

We'll cancel out in terms of the current direction and the velocity.

The question is, what happens if you're rolling right along?

I mean, I think in practice it's very unlikely, but in that case, yeah, you would just get

Get stuck in the saddle, yeah. I think that's like, you know, your initial conditions, like wherever you start.

Starr is very unfortunate. So yeah, sometimes I guess that could happen, but it's very unlikely.

Yeah, and that's also why in practice people won't run like a single model training run.

Often they'll run multiple ones with different random seeds just in case something like that could happen.

The other thing is, if you're doing stochastic gradient descent, you're much more likely to have at least a little bit of noise to get you out.

I've got it directly in that saddle back and forth. So I think basically it never would happen.

It could happen because of the randomness, but hypothetically, I think that could occur.

So the question is, why is the saddle just an issue with SGD and not optimization in general?

It might also be an issue with the entire dataset. It might even be more common with the entire dataset.

But it's also an issue that other optimization algorithms that just rely on gradient descent with no

No sort of bells and whistles attached would also, they would face the same thing.

Yeah, so the question is, does adding the momentum make it more difficult?

Is it difficult to converge because we'll overshoot and then you don't have to come back? I think the short answer is yeah, it might not

Not help with converging, but it will help you find, on average, it will help you find a better

So it will converge maybe more slowly because

Because you won't get stuck in a local minimum. You would just converge here if there was no momentum, right?

Versus Overshooting. So I think a lot of this stuff is empirically shown where it's like it has

I think it happens to be with the specific class of neural networks, the momentum does help training.

This is the intuition for why we prefer it. I think, to be honest,

People will use whatever works best, and there are cases where people have found that Stochastic Gradient

So here's the intuition about why.

We'll talk about why it could perform better, but in practice people will just try a bunch of different ones and see what works best.

And I'm going over the most common ones that people try now. But yeah, you're right. It could hurt convergence.

Interagents, potentially.

All right, I'll continue then.

I think we went through this. And one other thing I wanted to point out is that there are different ways

There are different ways you can formulate this, so these equations are identical, but you'll sometimes, depending on the implementation, see it written.

They're written in different ways, but they're doing the same thing. Maybe in the interest of time,

I'll skip over why they're identical, but you could go over in the slide and prove to yourself that these are essential.

They're essentially the same formulations. Okay, I think the next thing I'll talk about

What I want to talk about is a different optimizer. So we talked about momentum, and now we'll talk about something called RMS.

So RMSProp is a bit of an older method now.

Now, 2012, but it came out by Jeffrey Hinton's group.

The idea is to, instead of just having this running velocity,

The velocity, which the momentum captures, is to actually add element-wise scale.

Scaling of the Gradient. So how do we do this?

We have this gradient squared term. And the decay rate here is very much like the momentum that we expect.

The momentum term we explained before, but now it's on the squared gradient. So we have this.

We have this sort of running average where we take the previous term here, the gradient squared, and then we do 1 minus times and then here.

Here it is literally the gradient squared. And so this is a running average of our squared gradient.

So, you know, bigger values will get much bigger. Smaller values will get much smaller.

If there are consistently large gradients and certain values, those will get very large as we continue our...

So our running average here. And we're actually going to divide here. In the update step, we divide by

By the square root of it. So the basic idea here is we're actually now stepping, someone asked,

I think there was a question, what if we change the direction in which we're stepping? This is exactly the type of thing you can do.

And this is what this is doing, where we're dividing by this squared gradient term.

Values in which we have very large squared gradients for the values of W in which

In which the derivative is very large, we'll divide by a larger value.

Not as far in that direction. And the more flat regions will step farther because we're dividing by a smaller.

So this is the basic intuition behind it, and it very much addresses one of the questions someone asked.

Someone had earlier about, can we change the way we're stepping in the direction? And that's exactly what this is doing here. So you still have a learning rate.

But you're dividing it by this square root of the cumulative squared gradient.

Which gives you larger steps in the flatter areas of your lost landscape.

And shorter steps in the very steep areas. Can anyone explain? I sort of just gave a brief summary, but...

What happens in this specific line here of the code? Why does...

What happens with our gradient step direction? How does it change?

Dividing by this value, which is dependent on the current gradient and also the past gradients.

When these values are very large, so these are vector operations, so we have a set of

We have derivatives here, and we're dividing element-wise by another set of

We have squared gradient values. So when it's very large, the denominator is very large.

Then the step becomes effectively less in that direction because we're dividing by a large value. And when it's a very small value...

The step becomes much larger because the gradient squared term is small. So it's in the denominator.

And we're increasing the effective step size.

So it's specifically for this type of example here where you have maybe a very

Narrow Valley, where you want to be moving more in a flatter direction.

Yes, the question is, what does a small gradient mean in this context? And how does this help?

Help us move along the steep directions less and along the flat directions more.

Yeah, yeah, so I think actually this is maybe a great visual because it compares.

So it compares the three different approaches here. So we have with momentum, which you can see sort of an overshot.

Overshoots, as there was a question about earlier, but then it kind of comes back. You have SGD, which is slower because

Because it's just sort of always moving in the fixed direction. And then you have RMS prop, which we just mentioned.

The way that RMSprop works is because the gradient and the direction that I'm moving my mouse here

The gradients squared term is larger.

So we move less in that direction. So you can see it actually quickly

ReLU starts turning here towards the center where it's a flatter landscape at this point, but it's traversing.

More in that direction. So actually sort of changing the direction we're going based on going less in the steep direction and more.

More in the flat direction. So these are the sort of three, and then there's one more we'll discuss, which is by far the most

It's one of the most popular optimizer used in modern deep learning.

That's a combination of the STD Momentum and RMS Prop.

That's almost what the Atom Optimizer is, which is the most popular optimizer in deep learning.

How about the prerequisite knowledge now to understand it? So you look at it and this first

The first term here in the red is basically the momentum we described before, where

Where we have the current, so the beta one is like the momentum term.

And then we have the velocity here and we're taking a running average.

The second moment here is like the Gradient Squared term for RMSprop.

And we're doing the same thing here where we're multiplying the learning rate instead of by the step size.

By the velocity, but now we're still doing the thing where we take the square root.

And it's the second moment here. And the reason they use first moment and second moment is in relation to physics and mechanics.

But it's basically just a combination of the two things we explained earlier, where you're accelerating

Accelerating movement along the flat directions, dampening it among the steep ones, and you're also adding this notion of

of momentum and velocity so you gradually build up speed if you're continuously moving in the same direction.

So as it's written right now, this will actually run into issues.

At the very first time step. And it might be a little bit unclear.

Do you why? But I'll actually wait for someone to have a guess. So one thing to note is that these betas

Beta 1, Beta 2 are usually initialized very close to 1, so like 0.9, 0.999.

And that these two values are also initialized to zero.

First time step, if you just use this formulation of Adam, you would run into potentially unwanted behavior.

So one of the other things is it has to do with the second moment calculation.

So this is the main issue here. When you calculate the second moment and then

And use it on the next line, you sort of run into an issue.

So it starts at zero. So this term is zero. You have a very

Very large beta, so this value is very small, and if your gradient is not very

Very large in your first step, you can have this whole term basically be very close to zero. Now we're dividing by something very close to zero.

It just creates a very large initial step, even though our gradient was small.

So that's probably not something we really want. And so the final thing that Adam has is it adds these

We have these bias terms here, which is specifically to account for this issue where it's dependent now on the time step.

Step of training. So I think this is also something you'll go into in the homework.

I want to give the basically intuition behind Atom, why the naive implementation wouldn't work, which is...

Which is this really large initial step. And you'll go over in the homework implementing this and you'll see how this

I don't know how this time step is used, but the basic idea is this is to account for that very large initial step.

As your time step increases, these bias terms are not needed as much.

Okay, cool. These are some good defaults that people normally use.

If you're training a model with Atom, you go with these and maybe it'll work, maybe it won't.

Maybe it won't, but it's a good starting point. And you can then tell from the remaining

Slides, we'll talk about how do you know if your learning rate's right, how do you know if these other values are right.

So I'll speed up a little bit, just in the interest of time. But the basic idea is that you can see these all

All these different optimizers converging. They all have different properties. You can sort of see how Atom is this .

This combination of RMS prop and STD with momentum where it has characteristics of both, which is very neat.

To see visually it aligns with our intuition.

The final topic related to Adam is that we could look at how regularization interacts.

So for example, if we have L2 regularization, how does this affect how the optimizer works?

The optimizer works. And I think the answer is it's actually not immediately obvious, and you can do it.

They can do it in different ways. So in default Atom, they compute L2 when they're computing their gradient.

So, you know, we looked at the gradient and there was the last portion of our, so the data

Data Loss portion and then the regularization loss. For Atom, it's using both of those when it computes the gradient.

But Adam, W basically only looks at the data loss for doing all of these moment calculations.

And it just adds the regularization term at the end here.

So basically all I'm trying to describe to you all is there is flexibility for how you incorporate regularization into your

And your optimizers. Weight decay is generally when you just add it at the end. L2 regularization.

And you don't include it in the actual optimizer for how you're calculating the velocities.

So this is the main difference.

Under a lot of settings, AtomW works slightly better. I think the Llama series from Meta, they all use AtomW.

I assume because it does slightly better for them.

Yeah, so if you mix it into one function...

That's what Adam does. And Adam W is specifically separating it into two.

Why you might want to do that is because if you don't want your velocities, your momentums to

You want to actually be a function of the weights. You want it to be a function of the loss. So if you're trying to traverse your loss,

So if you want to make your class landscape more independent of your actual weight values, that's why you might want to separate it.

But you still might want a regularization term, but you don't want it to interfere with the moment calculation.

This is the specific reason why they do it. Ultimately, it's empirical. You try both and you see which one works better.

This is why you would do it that way. Okay, cool. So we'll talk about learning rates.

There are different ways in which learning rates can be chosen.

And sometimes you'll get a very high learning rate where what will happen is basically your loss will get very large.

As you sort of oscillate out of the lost landscape, as we described earlier, if you have a very low learning

Your issue is you just converge very slowly. If you have a high learning rate, but you're not oscillating out, but you might

You might not be able to converge because you're sort of bumping around the local minimum, but you're not actually able to .

You're not actually able to get any lower in because your learning rate's too high. And ideally, a good learning rate would have

You would have this property where it decreases, it causes your loss to decrease quickly over time, but then you see continued.

Continued improvements as you continue to train the model.

Actually, depending on the situation, a lot of these could be good learning rates and also depending on the step in training.

Which is the final...

...thing we'll discuss in lecture today. So you can actually change your learning rate.

As you train your model, you don't need to always have a fixed learning rate or step size.

Pretty much all modern deep learning, like all the best models coming out have different

They vary the learning rate during training. So one really simple way you could do it is

is after a fixed number of iterations, you just take one time

One-tenth of the learning rate and you continue training. So this can resolve the issue of where

Your learning rate is too high for you to be able to converge any further, so then you reduce it and you're able to

We're able to get lower into the lost landscape. And this is really commonly used when training Resnets.

So that's a very popular type of convolutional neural network, which we'll discuss later in the course.

Another thing you could do is sort of cosine learning rate decay.

So extremely popular. So here you have basically this is like how

Half of a cosine wave where you're starting at your maximum learning rate here, and then you go down to

To zero to the end. And it follows this sort of half cosine shape.

It's just the formula for calculating it. I won't go into any details, but the basic idea is there's a ton of different ways to do it.

When your loss uses a cosine learning rate scheduler, you'll often see a shape like this where

It's sort of, you get pretty good continued gains in the middle part of training, but the basic idea is

is that the actual shape of your loss during training will highly depend on what scheduler you use. So this is the basic idea.

It looks very different, for example, than this one, where you can literally see where we're taking

Taking one-tenth of the learning rate during training. Another thing you do is just a linear learning rate.

So it just follows a straight line. You could do inverse square root, et cetera, et cetera.

Basically, an unlimited number of ways you could

Mess with your learning rate during training.

And depending on the type of model you're training, and depending on what works best, you just choose the one that works best.

Here are some ones you could try that could perform well in your setting.

A really, really popular strategy is to have a linear warmup. So instead of just starting at your maximum learning rate,

You spend a fixed number of iterations to sort of linearly warm up to whatever you're

Whatever your maximum value is, and then you go about doing whatever scheduler you had afterwards. So, for example, linear warmup.

And then this would be like the inverse square root or linear warmup and then like cosine.

It's a very popular setup for training models. One final thing is that there is this

The first empirical rule of thumb are called the linear scaling.

Linear Scaling Hypothesis or Linear Scaling Law or something like that. I forget the name.

It's a linear scaling law, where it shows that if you increase your batch size or the number of training

Training examples per update by n. You should also scale your learning rate by n.

So as you increase your batch size, you should increase your learning rate.

Directly, proportionally. So I think the math behind this is a bit involved.

And also, it's more of an empirical rule of thumb. So people have tried to show mathematical

Technical proofs for why this could be useful, but based on the variation of gradients and your batch.

The number of gradients you calculate per batch, et cetera.

This is just shown empirically to be true for a large number of problems. So this is a good rule of thumb. If you have a winning recipe,

If you want to increase the batch size, then also increase your learning rate by the same amount.

Cool. And then the final thing I'll touch upon very briefly.

Basically, is this idea of second-order optimization, which is

It uses the Hessian that someone asked a question about earlier, too. So we won't talk about this very much in depth.

Just to let you know this exists, it's not something we cover in the course very much, but the basic idea is...

Right now we're using the gradient to form a linear approximation of basically where's the downward direction.

We're trying to traverse this lost landscape. And we just sort of

We look at the direction, and we take a general step in that direction. And we added fancy things like momentum and

The RMS prop where we're decelerating along steep directions, but this is the basic idea.

We're using this gradient at each time step.

The idea of the Hessian is instead of using the gradient, you basically try to fit a

A polynomial, a quadratic, or like a

Like a second degree polynomial to your function based on the derivatives at that point or the Hessians.

And you then try to find the minimum this way.

And again, optimization problems. This actually works extremely well. But generally, we don't use it in .

And deep learning because it requires two things. So one, you have to do this like Taylor series expansion.

Whereas right now we're just sort of doing the first part where we're taking the derivative, but you would need to be able to calculate.

The second mixed derivatives, which is already maybe difficult.

And on top of that, this mixed derivative of all of your

parameters in your model by all the other parameters in your model can get very large as you have like these

It's many million or billion parameter neural networks. So in practice, we don't use it because

Because these values have, the matrices become way too large.

You run out of memory on your computer if you try to run it. Specifically, you run out of GPU memory.

If you're a smaller model or if you're okay with spending much more time to get better.

If you want to do better steps towards your minimum, then maybe you want to look into this. Depends on the problem set.

But for smaller models, this actually works quite well. But for these large neural networks for training, we basically never do this due to the memory.

And all the time you spent computationally trying to calculate the hash

Etc., you would rather just see more data during training.

All right, so some, I guess, concluding...

Thoughts for you all that can be useful. So Adam or Adam W is a really good default choice.

If there's a choice for training your first model if you're working on a new problem in a domain, I would recommend it.

And it could even work okay even if you do constant learning rates. So usually people will try Adam, Adam.

Adam W's constant learning rate or with a linear warmup and then a cosine decay. Those are really popular.

Also, I think STD and Momentum can sometimes outperform.

Adam, but the tricky thing is because you generally have to

Tune the values more, so you have to try many more learning rates because you

You don't have this like RMS prop term to account for the steep directions and also you might

You might have to try different scheduling values, whereas in practice, atoms are like best-by-test. People have tried it in a bunch of different domains.

It works very well. It's very adaptive to the Loss landscape.

Like a full batch update where you're already at each step. You can fit basically your entire training set.

Into your batch size, you might want to look beyond first order optimization into second order.

So I'm going to go ahead and go ahead and go ahead and go ahead and go ahead and go ahead and go ahead and go ahead.

You could potentially benefit from having these non-linear sort of update steps.

And computing more sophisticated strategies for going down, trying to find the minimum.

So I think we're essentially done with the lecture.

I'll give some slides about looking forward. So how do we optimize more?

More complex functions than linear models, which is what we covered in this lecture. And next lecture specifically...

Basically we'll be looking at neural networks, which is a very exciting topic.

A two-layer neural network, the one we'll discuss in class, is basically you have two of these weight matrices.

There are two matrices now, one for each layer. And you have something called a non-linearity sort of.

Sort of stuck between. So in this case, the most common, sorry, not the most common, but the most simple

The simple one is just this ReLU function, which you'll learn about more. But the basic idea is now we have two weight matrices.

And we have this additional function that's done in between the weight matrix calculations.

This is nice because, as I said, it's nonlinear. So if we're trying to build a linear classifier to

If you have classified data like this, you'll run into an issue where the blue points and the red points are not linearly.

But maybe there's some transformations we can do, or through many layers of a model, we can eventually

And eventually transform the data into a way in which it is separable by a line, which would be our...

And sort of final layer of the model.